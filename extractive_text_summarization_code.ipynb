{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[],"machine_shape":"hm"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zS17q1uhs6H6"},"source":["**IMPORTING REQUIRED MODULES AND PACKAGES**"]},{"cell_type":"code","metadata":{"id":"mvMk4e4-s6IF","outputId":"94afe4b6-3fab-4395-e3d7-7714c1d2d8f7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701296579553,"user_tz":300,"elapsed":211,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["import numpy as np\n","import pandas as pd\n","import nltk\n","nltk.download('punkt') # one time execution"],"execution_count":117,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":117}]},{"cell_type":"markdown","metadata":{"id":"_IT6RGGTs6IZ"},"source":["**INPUT TEXT**"]},{"cell_type":"markdown","source":["The input text:\n","Extractive text summarization is a vital natural language processing (NLP) task aimed at condensing large volumes of textual information into concise summaries by selecting and assembling key sentences or passages. The process involves leveraging techniques from information retrieval, machine learning, and linguistic analysis to identify and rank the most significant content in a document. Methods such as TextRank and TF-IDF are commonly used to assess sentence importance based on features like word frequency, sentence position, and relationships between sentences. The objective is to create summaries that faithfully represent the primary ideas of the original text, providing users with a succinct overview without the need to delve into the entire document.\n","\n","Extractive summarization faces challenges in maintaining coherence, handling redundancy, and striking a balance between informativeness and brevity. Algorithms may use graph-based models, feature-based approaches, or machine learning models to score sentences, and they are evaluated using metrics like ROUGE. Real-time applications, such as news aggregation and social media content summarization, benefit from the speed and efficiency of extractive techniques. Ongoing advancements in neural networks, reinforcement learning, and hybrid models that combine extractive and abstractive approaches are shaping the future of extractive text summarization. The field's evolution continues to address limitations, making it a dynamic and critical area within NLP.\n","\n","Extractive text summarization finds applications across diverse domains, including news articles, legal documents, research papers, and online content. Its real-time capabilities make it valuable for time-sensitive information retrieval, allowing users to quickly access crucial details. The method is particularly relevant in scenarios where brevity is essential, providing decision-makers with summarized insights without the need for exhaustive reading. Redundancy mitigation techniques ensure that key information is not needlessly repeated, contributing to more concise and diverse summaries. Extractive summarization APIs and platforms equipped with pre-trained models democratize the use of summarization functionalities, enabling developers and businesses to integrate these tools seamlessly into their applications. As research in this field advances, there is a growing emphasis on addressing challenges, enhancing the accuracy of summaries, and exploring innovative approaches for improved outcomes in extractive text summarization."],"metadata":{"id":"LgF72SOuK5eD"}},{"cell_type":"code","metadata":{"id":"Mgwq3m0Ws6Ie","executionInfo":{"status":"ok","timestamp":1701296579553,"user_tz":300,"elapsed":5,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["s='Extractive text summarization is a vital natural language processing (NLP) task aimed at condensing large volumes of textual information into concise summaries by selecting and assembling key sentences or passages. The process involves leveraging techniques from information retrieval, machine learning, and linguistic analysis to identify and rank the most significant content in a document. Methods such as TextRank and TF-IDF are commonly used to assess sentence importance based on features like word frequency, sentence position, and relationships between sentences. The objective is to create summaries that faithfully represent the primary ideas of the original text, providing users with a succinct overview without the need to delve into the entire document.Extractive summarization faces challenges in maintaining coherence, handling redundancy, and striking a balance between informativeness and brevity. Algorithms may use graph-based models, feature-based approaches, or machine learning models to score sentences, and they are evaluated using metrics like ROUGE. Real-time applications, such as news aggregation and social media content summarization, benefit from the speed and efficiency of extractive techniques. Ongoing advancements in neural networks, reinforcement learning, and hybrid models that combine extractive and abstractive approaches are shaping the future of extractive text summarization. The fields evolution continues to address limitations, making it a dynamic and critical area within NLP.Extractive text summarization finds applications across diverse domains, including news articles, legal documents, research papers, and online content. Its real-time capabilities make it valuable for time-sensitive information retrieval, allowing users to quickly access crucial details. The method is particularly relevant in scenarios where brevity is essential, providing decision-makers with summarized insights without the need for exhaustive reading. Redundancy mitigation techniques ensure that key information is not needlessly repeated, contributing to more concise and diverse summaries. Extractive summarization APIs and platforms equipped with pre-trained models democratize the use of summarization functionalities, enabling developers and businesses to integrate these tools seamlessly into their applications. As research in this field advances, there is a growing emphasis on addressing challenges, enhancing the accuracy of summaries, and exploring innovative approaches for improved outcomes in extractive text summarization.'"],"execution_count":118,"outputs":[]},{"cell_type":"code","metadata":{"id":"CKq5Is5ys6Iw","outputId":"0c28ef52-a370-4637-f757-575021b74041","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701296579912,"user_tz":300,"elapsed":363,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["print(\"Input text:\",s)"],"execution_count":119,"outputs":[{"output_type":"stream","name":"stdout","text":["Input text: Extractive text summarization is a vital natural language processing (NLP) task aimed at condensing large volumes of textual information into concise summaries by selecting and assembling key sentences or passages. The process involves leveraging techniques from information retrieval, machine learning, and linguistic analysis to identify and rank the most significant content in a document. Methods such as TextRank and TF-IDF are commonly used to assess sentence importance based on features like word frequency, sentence position, and relationships between sentences. The objective is to create summaries that faithfully represent the primary ideas of the original text, providing users with a succinct overview without the need to delve into the entire document.Extractive summarization faces challenges in maintaining coherence, handling redundancy, and striking a balance between informativeness and brevity. Algorithms may use graph-based models, feature-based approaches, or machine learning models to score sentences, and they are evaluated using metrics like ROUGE. Real-time applications, such as news aggregation and social media content summarization, benefit from the speed and efficiency of extractive techniques. Ongoing advancements in neural networks, reinforcement learning, and hybrid models that combine extractive and abstractive approaches are shaping the future of extractive text summarization. The fields evolution continues to address limitations, making it a dynamic and critical area within NLP.Extractive text summarization finds applications across diverse domains, including news articles, legal documents, research papers, and online content. Its real-time capabilities make it valuable for time-sensitive information retrieval, allowing users to quickly access crucial details. The method is particularly relevant in scenarios where brevity is essential, providing decision-makers with summarized insights without the need for exhaustive reading. Redundancy mitigation techniques ensure that key information is not needlessly repeated, contributing to more concise and diverse summaries. Extractive summarization APIs and platforms equipped with pre-trained models democratize the use of summarization functionalities, enabling developers and businesses to integrate these tools seamlessly into their applications. As research in this field advances, there is a growing emphasis on addressing challenges, enhancing the accuracy of summaries, and exploring innovative approaches for improved outcomes in extractive text summarization.\n"]}]},{"cell_type":"markdown","metadata":{"id":"jp69uy2Ds6I8"},"source":["**TOKENIZATION OF SENTENCES**"]},{"cell_type":"code","metadata":{"id":"W1RzcqCRs6I_","executionInfo":{"status":"ok","timestamp":1701296579912,"user_tz":300,"elapsed":22,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["from nltk.tokenize import sent_tokenize\n","sentences=[]\n","sentences.append(sent_tokenize(s))\n","sentences = [word for sent in sentences for word in sent]"],"execution_count":120,"outputs":[]},{"cell_type":"code","metadata":{"id":"ml-j7cWMs6JJ","outputId":"06625206-9184-4bf3-c233-a9135466229f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701296579913,"user_tz":300,"elapsed":22,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["print(\"After tokenization:\",sentences)"],"execution_count":121,"outputs":[{"output_type":"stream","name":"stdout","text":["After tokenization: ['Extractive text summarization is a vital natural language processing (NLP) task aimed at condensing large volumes of textual information into concise summaries by selecting and assembling key sentences or passages.', 'The process involves leveraging techniques from information retrieval, machine learning, and linguistic analysis to identify and rank the most significant content in a document.', 'Methods such as TextRank and TF-IDF are commonly used to assess sentence importance based on features like word frequency, sentence position, and relationships between sentences.', 'The objective is to create summaries that faithfully represent the primary ideas of the original text, providing users with a succinct overview without the need to delve into the entire document.Extractive summarization faces challenges in maintaining coherence, handling redundancy, and striking a balance between informativeness and brevity.', 'Algorithms may use graph-based models, feature-based approaches, or machine learning models to score sentences, and they are evaluated using metrics like ROUGE.', 'Real-time applications, such as news aggregation and social media content summarization, benefit from the speed and efficiency of extractive techniques.', 'Ongoing advancements in neural networks, reinforcement learning, and hybrid models that combine extractive and abstractive approaches are shaping the future of extractive text summarization.', 'The fields evolution continues to address limitations, making it a dynamic and critical area within NLP.Extractive text summarization finds applications across diverse domains, including news articles, legal documents, research papers, and online content.', 'Its real-time capabilities make it valuable for time-sensitive information retrieval, allowing users to quickly access crucial details.', 'The method is particularly relevant in scenarios where brevity is essential, providing decision-makers with summarized insights without the need for exhaustive reading.', 'Redundancy mitigation techniques ensure that key information is not needlessly repeated, contributing to more concise and diverse summaries.', 'Extractive summarization APIs and platforms equipped with pre-trained models democratize the use of summarization functionalities, enabling developers and businesses to integrate these tools seamlessly into their applications.', 'As research in this field advances, there is a growing emphasis on addressing challenges, enhancing the accuracy of summaries, and exploring innovative approaches for improved outcomes in extractive text summarization.']\n"]}]},{"cell_type":"markdown","metadata":{"id":"y3fj1Eb9s6JV"},"source":["**REMOVE PUNCTUATIONS, NUMBERS AND SPECIAL CHARACTERS**"]},{"cell_type":"code","metadata":{"id":"wjtTMpQYs6JY","outputId":"c1dac11f-1e53-44a6-b5c4-5de2472b081e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701296579913,"user_tz":300,"elapsed":18,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["# remove punctuations, numbers and special characters\n","clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n","print(\"clean sentences:\")\n","print(clean_sentences)"],"execution_count":122,"outputs":[{"output_type":"stream","name":"stdout","text":["clean sentences:\n","0     Extractive text summarization is a vital natur...\n","1     The process involves leveraging techniques fro...\n","2     Methods such as TextRank and TF IDF are common...\n","3     The objective is to create summaries that fait...\n","4     Algorithms may use graph based models  feature...\n","5     Real time applications  such as news aggregati...\n","6     Ongoing advancements in neural networks  reinf...\n","7     The fields evolution continues to address limi...\n","8     Its real time capabilities make it valuable fo...\n","9     The method is particularly relevant in scenari...\n","10    Redundancy mitigation techniques ensure that k...\n","11    Extractive summarization APIs and platforms eq...\n","12    As research in this field advances  there is a...\n","dtype: object\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-122-46f06ae4f408>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n","  clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n"]}]},{"cell_type":"code","metadata":{"id":"3h8JicJQs6Jj","executionInfo":{"status":"ok","timestamp":1701296579913,"user_tz":300,"elapsed":14,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["# make alphabets lowercase\n","clean_sentences = [s.lower() for s in clean_sentences]"],"execution_count":123,"outputs":[]},{"cell_type":"code","metadata":{"id":"VrVBEc0us6Jz","outputId":"f68119af-39a5-47fd-a631-83fa5ea261ec","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701296579913,"user_tz":300,"elapsed":13,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["print(clean_sentences)"],"execution_count":124,"outputs":[{"output_type":"stream","name":"stdout","text":["['extractive text summarization is a vital natural language processing  nlp  task aimed at condensing large volumes of textual information into concise summaries by selecting and assembling key sentences or passages ', 'the process involves leveraging techniques from information retrieval  machine learning  and linguistic analysis to identify and rank the most significant content in a document ', 'methods such as textrank and tf idf are commonly used to assess sentence importance based on features like word frequency  sentence position  and relationships between sentences ', 'the objective is to create summaries that faithfully represent the primary ideas of the original text  providing users with a succinct overview without the need to delve into the entire document extractive summarization faces challenges in maintaining coherence  handling redundancy  and striking a balance between informativeness and brevity ', 'algorithms may use graph based models  feature based approaches  or machine learning models to score sentences  and they are evaluated using metrics like rouge ', 'real time applications  such as news aggregation and social media content summarization  benefit from the speed and efficiency of extractive techniques ', 'ongoing advancements in neural networks  reinforcement learning  and hybrid models that combine extractive and abstractive approaches are shaping the future of extractive text summarization ', 'the fields evolution continues to address limitations  making it a dynamic and critical area within nlp extractive text summarization finds applications across diverse domains  including news articles  legal documents  research papers  and online content ', 'its real time capabilities make it valuable for time sensitive information retrieval  allowing users to quickly access crucial details ', 'the method is particularly relevant in scenarios where brevity is essential  providing decision makers with summarized insights without the need for exhaustive reading ', 'redundancy mitigation techniques ensure that key information is not needlessly repeated  contributing to more concise and diverse summaries ', 'extractive summarization apis and platforms equipped with pre trained models democratize the use of summarization functionalities  enabling developers and businesses to integrate these tools seamlessly into their applications ', 'as research in this field advances  there is a growing emphasis on addressing challenges  enhancing the accuracy of summaries  and exploring innovative approaches for improved outcomes in extractive text summarization ']\n"]}]},{"cell_type":"markdown","metadata":{"id":"6qCOg8Ges6J_"},"source":["**REMOVAL OF STOPWORDS**"]},{"cell_type":"code","metadata":{"id":"iIFb7j9Ts6KC","outputId":"c609fe75-4b1f-4e7b-f8ee-9182f9b375f5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701296580158,"user_tz":300,"elapsed":254,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","print(\"before removing stop words\")\n","print(clean_sentences)\n","def remove_stopwords(sen):\n","    sample_list=[]\n","    for i in sen:\n","      if i not in stop_words:\n","        sample_list.append(i)\n","    sen_new=\" \".join(sample_list)\n","    return sen_new\n","clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n","print(\"removing stop words:\")\n","print(clean_sentences)"],"execution_count":125,"outputs":[{"output_type":"stream","name":"stdout","text":["before removing stop words\n","['extractive text summarization is a vital natural language processing  nlp  task aimed at condensing large volumes of textual information into concise summaries by selecting and assembling key sentences or passages ', 'the process involves leveraging techniques from information retrieval  machine learning  and linguistic analysis to identify and rank the most significant content in a document ', 'methods such as textrank and tf idf are commonly used to assess sentence importance based on features like word frequency  sentence position  and relationships between sentences ', 'the objective is to create summaries that faithfully represent the primary ideas of the original text  providing users with a succinct overview without the need to delve into the entire document extractive summarization faces challenges in maintaining coherence  handling redundancy  and striking a balance between informativeness and brevity ', 'algorithms may use graph based models  feature based approaches  or machine learning models to score sentences  and they are evaluated using metrics like rouge ', 'real time applications  such as news aggregation and social media content summarization  benefit from the speed and efficiency of extractive techniques ', 'ongoing advancements in neural networks  reinforcement learning  and hybrid models that combine extractive and abstractive approaches are shaping the future of extractive text summarization ', 'the fields evolution continues to address limitations  making it a dynamic and critical area within nlp extractive text summarization finds applications across diverse domains  including news articles  legal documents  research papers  and online content ', 'its real time capabilities make it valuable for time sensitive information retrieval  allowing users to quickly access crucial details ', 'the method is particularly relevant in scenarios where brevity is essential  providing decision makers with summarized insights without the need for exhaustive reading ', 'redundancy mitigation techniques ensure that key information is not needlessly repeated  contributing to more concise and diverse summaries ', 'extractive summarization apis and platforms equipped with pre trained models democratize the use of summarization functionalities  enabling developers and businesses to integrate these tools seamlessly into their applications ', 'as research in this field advances  there is a growing emphasis on addressing challenges  enhancing the accuracy of summaries  and exploring innovative approaches for improved outcomes in extractive text summarization ']\n","removing stop words:\n","['extractive text summarization vital natural language processing nlp task aimed condensing large volumes textual information concise summaries selecting assembling key sentences passages', 'process involves leveraging techniques information retrieval machine learning linguistic analysis identify rank significant content document', 'methods textrank tf idf commonly used assess sentence importance based features like word frequency sentence position relationships sentences', 'objective create summaries faithfully represent primary ideas original text providing users succinct overview without need delve entire document extractive summarization faces challenges maintaining coherence handling redundancy striking balance informativeness brevity', 'algorithms may use graph based models feature based approaches machine learning models score sentences evaluated using metrics like rouge', 'real time applications news aggregation social media content summarization benefit speed efficiency extractive techniques', 'ongoing advancements neural networks reinforcement learning hybrid models combine extractive abstractive approaches shaping future extractive text summarization', 'fields evolution continues address limitations making dynamic critical area within nlp extractive text summarization finds applications across diverse domains including news articles legal documents research papers online content', 'real time capabilities make valuable time sensitive information retrieval allowing users quickly access crucial details', 'method particularly relevant scenarios brevity essential providing decision makers summarized insights without need exhaustive reading', 'redundancy mitigation techniques ensure key information needlessly repeated contributing concise diverse summaries', 'extractive summarization apis platforms equipped pre trained models democratize use summarization functionalities enabling developers businesses integrate tools seamlessly applications', 'research field advances growing emphasis addressing challenges enhancing accuracy summaries exploring innovative approaches improved outcomes extractive text summarization']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"markdown","metadata":{"id":"qj5MEDEns6KN"},"source":["**LEMMATIZATION OF SENTENCES**"]},{"cell_type":"code","metadata":{"id":"kCcNSWeRs6KP","outputId":"bbf1d722-a119-4056-f55c-142a8bd027f0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701296580158,"user_tz":300,"elapsed":29,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","print(\"Before lemmatization:\")\n","print(clean_sentences)\n","\n","from nltk import pos_tag\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","\n","part = {\n","    'N' : 'n',\n","    'V' : 'v',\n","    'J' : 'a',\n","    'R' : 'r'\n","}\n","\n","wnl = WordNetLemmatizer()\n","\n","def convert_tag(penn_tag):\n","    if penn_tag in part.keys():\n","        return part[penn_tag]\n","    else:\n","        return 'n'\n","\n","\n","def tag_and_lem(element):\n","    sent = pos_tag(word_tokenize(element))\n","    return ' '.join([wnl.lemmatize(sent[k][0], convert_tag(sent[k][1][0]))\n","                    for k in range(len(sent))])\n","result_sentence=[]\n","for i in clean_sentences:\n","    value=tag_and_lem(i)\n","    result_sentence.append(value)\n","lemmatized_sentences=result_sentence\n","print(\"After lemmatization:\")\n","print(lemmatized_sentences)"],"execution_count":126,"outputs":[{"output_type":"stream","name":"stdout","text":["Before lemmatization:\n","['extractive text summarization vital natural language processing nlp task aimed condensing large volumes textual information concise summaries selecting assembling key sentences passages', 'process involves leveraging techniques information retrieval machine learning linguistic analysis identify rank significant content document', 'methods textrank tf idf commonly used assess sentence importance based features like word frequency sentence position relationships sentences', 'objective create summaries faithfully represent primary ideas original text providing users succinct overview without need delve entire document extractive summarization faces challenges maintaining coherence handling redundancy striking balance informativeness brevity', 'algorithms may use graph based models feature based approaches machine learning models score sentences evaluated using metrics like rouge', 'real time applications news aggregation social media content summarization benefit speed efficiency extractive techniques', 'ongoing advancements neural networks reinforcement learning hybrid models combine extractive abstractive approaches shaping future extractive text summarization', 'fields evolution continues address limitations making dynamic critical area within nlp extractive text summarization finds applications across diverse domains including news articles legal documents research papers online content', 'real time capabilities make valuable time sensitive information retrieval allowing users quickly access crucial details', 'method particularly relevant scenarios brevity essential providing decision makers summarized insights without need exhaustive reading', 'redundancy mitigation techniques ensure key information needlessly repeated contributing concise diverse summaries', 'extractive summarization apis platforms equipped pre trained models democratize use summarization functionalities enabling developers businesses integrate tools seamlessly applications', 'research field advances growing emphasis addressing challenges enhancing accuracy summaries exploring innovative approaches improved outcomes extractive text summarization']\n","After lemmatization:\n","['extractive text summarization vital natural language processing nlp task aim condense large volume textual information concise summary select assemble key sentence passage', 'process involve leverage technique information retrieval machine learn linguistic analysis identify rank significant content document', 'method textrank tf idf commonly use assess sentence importance base feature like word frequency sentence position relationship sentence', 'objective create summary faithfully represent primary idea original text provide user succinct overview without need delve entire document extractive summarization face challenge maintain coherence handle redundancy strike balance informativeness brevity', 'algorithm may use graph base model feature base approach machine learn model score sentence evaluate use metric like rouge', 'real time application news aggregation social medium content summarization benefit speed efficiency extractive technique', 'ongoing advancement neural network reinforcement learn hybrid model combine extractive abstractive approach shape future extractive text summarization', 'field evolution continue address limitation make dynamic critical area within nlp extractive text summarization find application across diverse domain include news article legal document research paper online content', 'real time capability make valuable time sensitive information retrieval allow user quickly access crucial detail', 'method particularly relevant scenario brevity essential provide decision maker summarize insight without need exhaustive reading', 'redundancy mitigation technique ensure key information needlessly repeat contribute concise diverse summary', 'extractive summarization apis platform equip pre trained model democratize use summarization functionality enable developer business integrate tool seamlessly application', 'research field advance grow emphasis address challenge enhance accuracy summary explore innovative approach improve outcomes extractive text summarization']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","metadata":{"id":"Cyi2qeFQs6KZ","outputId":"42e98155-112e-4e51-e2a9-797e8ef1bc74","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701296580158,"user_tz":300,"elapsed":26,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["len(lemmatized_sentences)"],"execution_count":127,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13"]},"metadata":{},"execution_count":127}]},{"cell_type":"markdown","metadata":{"id":"Umz9Tlp6s6Kn"},"source":["**SIMILARITY OF SENTENCES USING COSINE SIMILARITY**"]},{"cell_type":"code","metadata":{"id":"1EbAB3g2s6Ky","executionInfo":{"status":"ok","timestamp":1701296580158,"user_tz":300,"elapsed":22,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["import math"],"execution_count":128,"outputs":[]},{"cell_type":"code","metadata":{"id":"3HfyEAq-s6K-","executionInfo":{"status":"ok","timestamp":1701296580158,"user_tz":300,"elapsed":21,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["sim_mat = np.zeros([len(sentences), len(sentences)])"],"execution_count":129,"outputs":[]},{"cell_type":"code","metadata":{"id":"cDUk1UZjs6LJ","outputId":"7050dfe8-c4d3-4edc-edd0-a9326ae7cf94","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701296580159,"user_tz":300,"elapsed":22,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["sim_mat"],"execution_count":130,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"]},"metadata":{},"execution_count":130}]},{"cell_type":"code","metadata":{"id":"UDq5SmyUs6LW","executionInfo":{"status":"ok","timestamp":1701296580159,"user_tz":300,"elapsed":19,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["#similarity_matrix=[]\n","for x in range(0,len(lemmatized_sentences)):\n","   # sim_mat=[]\n","    s1=lemmatized_sentences[x]\n","    s1_words=s1.split()\n","    for y in range(0,len(lemmatized_sentences)):\n","        s2=lemmatized_sentences[y]\n","        s2_words=s2.split()\n","        unique_words=[]\n","        for word in s1_words:\n","            if word not in unique_words:\n","                unique_words.append(word)\n","        for word in s2_words:\n","            if word not in unique_words:\n","                unique_words.append(word)\n","        d={}\n","        for j in unique_words:\n","            d[j]=0\n","        d1=[]\n","        d2=[]\n","        for k in d.keys():\n","            d1.append(s1_words.count(k))\n","            d2.append(s2_words.count(k))\n","        sum_d1=0\n","        for i in d1:\n","            sum_d1=sum_d1+i*i\n","            s1_norm=math.sqrt(sum_d1)\n","        sum_d2=0\n","        for i in d2:\n","            sum_d2=sum_d2+i*i\n","            s2_norm=math.sqrt(sum_d2)\n","        similarity_value=0\n","        for i in range(0,len(d1)):\n","            d1[i]=d1[i]/s1_norm\n","            d2[i]=d2[i]/s2_norm\n","            similarity_value+=d1[i]*d2[i]\n","        sim_mat[x][y]=similarity_value"],"execution_count":131,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q9xsl14Ks6Li","outputId":"6ef51c31-875f-40ef-edad-4c49871600a7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701296580159,"user_tz":300,"elapsed":19,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["sim_mat"],"execution_count":132,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.        , 0.05504819, 0.13055824, 0.15569979, 0.04264014,\n","        0.11396058, 0.1956464 , 0.16116459, 0.05170877, 0.        ,\n","        0.24618298, 0.13957263, 0.20100756],\n","       [0.05504819, 1.        , 0.        , 0.04714045, 0.10327956,\n","        0.13801311, 0.05923489, 0.09759001, 0.12524486, 0.        ,\n","        0.1490712 , 0.        , 0.        ],\n","       [0.13055824, 0.        , 1.        , 0.        , 0.36742346,\n","        0.        , 0.        , 0.        , 0.        , 0.05270463,\n","        0.        , 0.04454354, 0.        ],\n","       [0.15569979, 0.04714045, 0.        , 1.        , 0.        ,\n","        0.09759001, 0.16754156, 0.13801311, 0.04428074, 0.18856181,\n","        0.10540926, 0.11952286, 0.21516574],\n","       [0.04264014, 0.10327956, 0.36742346, 0.        , 1.        ,\n","        0.        , 0.18353259, 0.        , 0.        , 0.        ,\n","        0.        , 0.17457431, 0.04714045],\n","       [0.11396058, 0.13801311, 0.        , 0.09759001, 0.        ,\n","        1.        , 0.1839418 , 0.25253814, 0.19446112, 0.        ,\n","        0.07715167, 0.23328474, 0.12598816],\n","       [0.1956464 , 0.05923489, 0.        , 0.16754156, 0.18353259,\n","        0.1839418 , 1.        , 0.17342199, 0.        , 0.        ,\n","        0.        , 0.25031309, 0.27036904],\n","       [0.16116459, 0.09759001, 0.        , 0.13801311, 0.        ,\n","        0.25253814, 0.17342199, 1.        , 0.04583492, 0.        ,\n","        0.05455447, 0.16495722, 0.26726124],\n","       [0.05170877, 0.12524486, 0.        , 0.04428074, 0.        ,\n","        0.19446112, 0.        , 0.04583492, 1.        , 0.        ,\n","        0.070014  , 0.        , 0.        ],\n","       [0.        , 0.        , 0.05270463, 0.18856181, 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 1.        ,\n","        0.        , 0.        , 0.        ],\n","       [0.24618298, 0.1490712 , 0.        , 0.10540926, 0.        ,\n","        0.07715167, 0.        , 0.05455447, 0.070014  , 0.        ,\n","        1.        , 0.        , 0.06804138],\n","       [0.13957263, 0.        , 0.04454354, 0.11952286, 0.17457431,\n","        0.23328474, 0.25031309, 0.16495722, 0.        , 0.        ,\n","        0.        , 1.        , 0.15430335],\n","       [0.20100756, 0.        , 0.        , 0.21516574, 0.04714045,\n","        0.12598816, 0.27036904, 0.26726124, 0.        , 0.        ,\n","        0.06804138, 0.15430335, 1.        ]])"]},"metadata":{},"execution_count":132}]},{"cell_type":"code","metadata":{"id":"kfcIFEZls6Lu","executionInfo":{"status":"ok","timestamp":1701296580159,"user_tz":300,"elapsed":16,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["m=len(sim_mat)"],"execution_count":133,"outputs":[]},{"cell_type":"code","metadata":{"id":"a35YUlyZs6L7","outputId":"daf69759-6393-4935-89a6-ea4c18eee137","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701296580159,"user_tz":300,"elapsed":15,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["print(m)"],"execution_count":134,"outputs":[{"output_type":"stream","name":"stdout","text":["13\n"]}]},{"cell_type":"markdown","metadata":{"id":"BaZOK51E6jyr"},"source":["**CALCULATING TEXTRANK FOR SENTENCES**"]},{"cell_type":"code","metadata":{"id":"PboxoxMstXk1","outputId":"e79341b3-3b4b-4809-8cea-115f288ed594","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701296580160,"user_tz":300,"elapsed":12,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["damping_factor_matrix = []\n","\n","for i in range(m):\n","  a=[]\n","  for j in range(1):\n","    a.append(0.85)\n","  damping_factor_matrix.append(a)\n","print(damping_factor_matrix)"],"execution_count":135,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.85], [0.85], [0.85], [0.85], [0.85], [0.85], [0.85], [0.85], [0.85], [0.85], [0.85], [0.85], [0.85]]\n"]}]},{"cell_type":"code","metadata":{"id":"wgOhuxjXtmjX","outputId":"4ec9c914-2411-4ba1-b8f3-d45ee30687f3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701296580160,"user_tz":300,"elapsed":10,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["for i in range(m):\n","    for j in range(1):\n","        print(damping_factor_matrix[i][j], end = \" \")\n","    print()"],"execution_count":136,"outputs":[{"output_type":"stream","name":"stdout","text":["0.85 \n","0.85 \n","0.85 \n","0.85 \n","0.85 \n","0.85 \n","0.85 \n","0.85 \n","0.85 \n","0.85 \n","0.85 \n","0.85 \n","0.85 \n"]}]},{"cell_type":"code","metadata":{"id":"Cpr3ozhGwV4g","executionInfo":{"status":"ok","timestamp":1701296580420,"user_tz":300,"elapsed":267,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["transpose_matrix=np.transpose(sim_mat)"],"execution_count":137,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZaOZUrTmuwcy","executionInfo":{"status":"ok","timestamp":1701296580420,"user_tz":300,"elapsed":15,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["for k in range(3):\n","  res = np.dot(transpose_matrix,damping_factor_matrix)\n","  damping_factor_matrix=res"],"execution_count":138,"outputs":[]},{"cell_type":"code","metadata":{"id":"SqJz25aexlOZ","outputId":"efb42378-61c8-491b-fc28-5f670ef00a85","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701296580420,"user_tz":300,"elapsed":14,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["print(res)"],"execution_count":139,"outputs":[{"output_type":"stream","name":"stdout","text":["[[10.75817698]\n"," [ 5.78931206]\n"," [ 4.45560132]\n"," [ 9.29758771]\n"," [ 6.48513699]\n"," [10.41245592]\n"," [11.44607289]\n"," [10.57803548]\n"," [ 4.40609073]\n"," [ 2.41741714]\n"," [ 6.10673794]\n"," [10.1264003 ]\n"," [10.78217957]]\n"]}]},{"cell_type":"code","metadata":{"id":"G8NxvN0pv3vV","outputId":"832f6fee-7e70-441a-9c14-96a796b0b675","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701296580421,"user_tz":300,"elapsed":13,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["rank_dict={}\n","for s in range(len(sentences)):\n","  rank_dict[s]=res[s][0]\n","print(rank_dict)"],"execution_count":140,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 10.75817697748218, 1: 5.789312060016041, 2: 4.455601315427076, 3: 9.29758771103963, 4: 6.485136994735127, 5: 10.41245591633683, 6: 11.446072894629252, 7: 10.578035480653654, 8: 4.4060907294096605, 9: 2.4174171388628727, 10: 6.106737939733235, 11: 10.126400304103619, 12: 10.782179574959532}\n"]}]},{"cell_type":"markdown","metadata":{"id":"FUnZDvlh6zj7"},"source":["**SORTING THE SENTENCES BASED ON THEIR RANKS**"]},{"cell_type":"code","metadata":{"id":"_IpAiNWhwBdH","outputId":"9bcac005-9c27-47ea-9b61-afcbf862ce10","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701296580421,"user_tz":300,"elapsed":10,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["ranked_sentences = sorted(((rank_dict[i],s) for i,s in enumerate(sentences)), reverse=True)\n","print(ranked_sentences)"],"execution_count":141,"outputs":[{"output_type":"stream","name":"stdout","text":["[(11.446072894629252, 'Ongoing advancements in neural networks, reinforcement learning, and hybrid models that combine extractive and abstractive approaches are shaping the future of extractive text summarization.'), (10.782179574959532, 'As research in this field advances, there is a growing emphasis on addressing challenges, enhancing the accuracy of summaries, and exploring innovative approaches for improved outcomes in extractive text summarization.'), (10.75817697748218, 'Extractive text summarization is a vital natural language processing (NLP) task aimed at condensing large volumes of textual information into concise summaries by selecting and assembling key sentences or passages.'), (10.578035480653654, 'The fields evolution continues to address limitations, making it a dynamic and critical area within NLP.Extractive text summarization finds applications across diverse domains, including news articles, legal documents, research papers, and online content.'), (10.41245591633683, 'Real-time applications, such as news aggregation and social media content summarization, benefit from the speed and efficiency of extractive techniques.'), (10.126400304103619, 'Extractive summarization APIs and platforms equipped with pre-trained models democratize the use of summarization functionalities, enabling developers and businesses to integrate these tools seamlessly into their applications.'), (9.29758771103963, 'The objective is to create summaries that faithfully represent the primary ideas of the original text, providing users with a succinct overview without the need to delve into the entire document.Extractive summarization faces challenges in maintaining coherence, handling redundancy, and striking a balance between informativeness and brevity.'), (6.485136994735127, 'Algorithms may use graph-based models, feature-based approaches, or machine learning models to score sentences, and they are evaluated using metrics like ROUGE.'), (6.106737939733235, 'Redundancy mitigation techniques ensure that key information is not needlessly repeated, contributing to more concise and diverse summaries.'), (5.789312060016041, 'The process involves leveraging techniques from information retrieval, machine learning, and linguistic analysis to identify and rank the most significant content in a document.'), (4.455601315427076, 'Methods such as TextRank and TF-IDF are commonly used to assess sentence importance based on features like word frequency, sentence position, and relationships between sentences.'), (4.4060907294096605, 'Its real-time capabilities make it valuable for time-sensitive information retrieval, allowing users to quickly access crucial details.'), (2.4174171388628727, 'The method is particularly relevant in scenarios where brevity is essential, providing decision-makers with summarized insights without the need for exhaustive reading.')]\n"]}]},{"cell_type":"markdown","metadata":{"id":"L_RYQMCf66VV"},"source":["**PRINTING THE TOP MOST IMPORTANT SENTENCES IN A PARAGRAPH**"]},{"cell_type":"code","metadata":{"id":"gogq-i8k6b1a","outputId":"7ab1d5fb-6802-4e23-d129-7f78097f8ed2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701296580421,"user_tz":300,"elapsed":7,"user":{"displayName":"Bhuvana Sri Likhitha Kanakam","userId":"00442878222826598641"}}},"source":["for i in range(5):\n","    print(ranked_sentences[i][1])"],"execution_count":142,"outputs":[{"output_type":"stream","name":"stdout","text":["Ongoing advancements in neural networks, reinforcement learning, and hybrid models that combine extractive and abstractive approaches are shaping the future of extractive text summarization.\n","As research in this field advances, there is a growing emphasis on addressing challenges, enhancing the accuracy of summaries, and exploring innovative approaches for improved outcomes in extractive text summarization.\n","Extractive text summarization is a vital natural language processing (NLP) task aimed at condensing large volumes of textual information into concise summaries by selecting and assembling key sentences or passages.\n","The fields evolution continues to address limitations, making it a dynamic and critical area within NLP.Extractive text summarization finds applications across diverse domains, including news articles, legal documents, research papers, and online content.\n","Real-time applications, such as news aggregation and social media content summarization, benefit from the speed and efficiency of extractive techniques.\n"]}]}]}